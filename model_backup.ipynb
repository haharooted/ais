{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## create trajectories\n",
    "import pyarrow.csv as pc\n",
    "import pyarrow as pa\n",
    "\n",
    "# Define the path to your CSV file\n",
    "csv_file = \"./data/cleaned/2.csv\"  # Change this to your actual file path\n",
    "\n",
    "# Read the CSV file into a PyArrow Table\n",
    "try:\n",
    "    table = pc.read_csv(csv_file)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading CSV file: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Convert to a pandas DataFrame\n",
    "df = table.to_pandas()\n",
    "\n",
    "# Display first few rows\n",
    "print(df.head())\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df.columns\n",
    "\n",
    "df.to_csv(\"./data/swag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing and evaluation libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "# Machine learning models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# For SMOTE to address class imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# =============================================================================\n",
    "# 1. Data Preparation and Preprocessing\n",
    "# =============================================================================\n",
    "# Assume that the original dataframe \"df\" is already loaded in your environment.\n",
    "# For example, you might have read it from a CSV:\n",
    "# df = pd.read_csv(\"your_data.csv\")\n",
    "\n",
    "# ----- Filter the Dataset by Ship Type -----\n",
    "# Here we choose specific ship types to include.\n",
    "wanted_ship_types = ['Cargo', 'Tanker', 'Fishing', 'HSC', 'Passenger']  # <-- Change these as needed\n",
    "df_filtered = df[df['Ship type'].isin(wanted_ship_types)]\n",
    "\n",
    "# Make a copy of the filtered dataframe\n",
    "df_clean = df_filtered.copy()\n",
    "\n",
    "# ----- Remove Unnecessary Columns -----\n",
    "columns_to_drop = [\"Name\", \"Callsign\", \"Destination\", \"MMSI\", \"trip_id\", \"trip_start\", \"trip_end\"]\n",
    "df_clean = df_clean.drop(columns=columns_to_drop)\n",
    "\n",
    "# ----- Encode Categorical Variables -----\n",
    "# We encode columns such as \"Ship type\" and other categorical features.\n",
    "cat_columns = [\"Type of mobile\", \"Ship type\", \"Cargo type\"]\n",
    "le_dict = {}\n",
    "for col in cat_columns:\n",
    "    le = LabelEncoder()\n",
    "    # Convert to string in case any labels are numeric already\n",
    "    df_clean[col] = le.fit_transform(df_clean[col].astype(str))\n",
    "    le_dict[col] = le\n",
    "\n",
    "# ----- Check and Handle Missing Values -----\n",
    "print(\"Missing values per column:\")\n",
    "print(df_clean.isna().sum())\n",
    "# Here we simply drop rows with any missing values.\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# ----- Separate Features and Target -----\n",
    "# We want to predict \"Ship type\", so we separate it as the target.\n",
    "y = df_clean[\"Ship type\"]             # Target variable (numeric encoding of ship type)\n",
    "X = df_clean.drop(columns=[\"Ship type\"])  # All other columns are features\n",
    "\n",
    "# ----- Feature Scaling -----\n",
    "# Scale numeric features so that each feature contributes equally.\n",
    "scaler = StandardScaler()\n",
    "numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "X[numeric_columns] = scaler.fit_transform(X[numeric_columns])\n",
    "\n",
    "# ----- (Optional) Print the Mapping of Labels to Original Ship Types -----\n",
    "le_ship_type = le_dict[\"Ship type\"]\n",
    "ship_type_mapping = dict(enumerate(le_ship_type.classes_))\n",
    "print(\"\\nMapping of numeric labels to original ship type names:\")\n",
    "print(ship_type_mapping)\n",
    "\n",
    "# =============================================================================\n",
    "# 2. Split Data into Training and Test Sets\n",
    "# =============================================================================\n",
    "# We use stratification to ensure that the class distribution is similar in both sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "print(\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts())\n",
    "print(\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts())\n",
    "\n",
    "# =============================================================================\n",
    "# 3. Define a Helper Function for Model Evaluation\n",
    "# =============================================================================\n",
    "def evaluate_classifier(model, X_eval, y_eval, classifier_name):\n",
    "    \"\"\"\n",
    "    Evaluate a classifier on the given evaluation set and print key metrics.\n",
    "    This function prints accuracy, macro-averaged precision, recall, F1-score,\n",
    "    and a detailed classification report.\n",
    "    \n",
    "    Parameters:\n",
    "        model: A trained model that supports the .predict() method.\n",
    "        X_eval: The feature set for evaluation.\n",
    "        y_eval: The true labels.\n",
    "        classifier_name: String name for the classifier (used in print statements).\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary containing the evaluation metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_eval)\n",
    "    \n",
    "    acc  = accuracy_score(y_eval, y_pred)\n",
    "    prec = precision_score(y_eval, y_pred, average='macro', zero_division=0)\n",
    "    rec  = recall_score(y_eval, y_pred, average='macro', zero_division=0)\n",
    "    f1   = f1_score(y_eval, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    print(f\"\\nClassifier: {classifier_name}\")\n",
    "    print(f\"Accuracy:         {acc:.4f}\")\n",
    "    print(f\"Precision (macro): {prec:.4f}\")\n",
    "    print(f\"Recall (macro):    {rec:.4f}\")\n",
    "    print(f\"F1 Score (macro):  {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_eval, y_pred, zero_division=0))\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    return {\n",
    "        \"Classifier\": classifier_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Precision\": prec,\n",
    "        \"Recall\": rec,\n",
    "        \"F1\": f1\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# 4. Setup Cross-Validation\n",
    "# =============================================================================\n",
    "# We use StratifiedKFold to ensure each fold has a similar class distribution.\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# =============================================================================\n",
    "# 5. Evaluate Default Models using Cross-Validation on the Training Set\n",
    "# =============================================================================\n",
    "# We define a dictionary of default models.\n",
    "default_models = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"SVM\": SVC(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "print(\"\\n=== CROSS-VALIDATION SCORES (DEFAULT PARAMETERS) on Original Training Data ===\")\n",
    "for name, model in default_models.items():\n",
    "    # Using accuracy as the scoring metric; you could also try others.\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "    print(f\"{name} - CV Accuracy: {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "# =============================================================================\n",
    "# 6. Train Default Models on the Entire Training Set and Evaluate on the Test Set\n",
    "# =============================================================================\n",
    "results = []  # To store results for summary\n",
    "\n",
    "print(\"\\n=== TEST SET EVALUATION (DEFAULT MODELS on Original Data) ===\")\n",
    "for name, model in default_models.items():\n",
    "    # Train the model on the full training data\n",
    "    model.fit(X_train, y_train)\n",
    "    # Evaluate on the test set and record the results\n",
    "    result = evaluate_classifier(model, X_test, y_test, name + \" (Default)\")\n",
    "    results.append(result)\n",
    "\n",
    "# =============================================================================\n",
    "# 7. Hyperparameter Tuning via GridSearchCV (Using Cross-Validation)\n",
    "# =============================================================================\n",
    "# We tune only SVM, Decision Tree, and Random Forest.\n",
    "# Grid search will try different combinations of parameters and choose the best one based on cross-validation performance.\n",
    "\n",
    "# --- Define parameter grids for each model ---\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "param_grid_dt = {\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# --- Setup GridSearchCV for each tuned model ---\n",
    "grid_svm = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_dt  = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_rf  = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "print(\"\\n=== HYPERPARAMETER TUNING (Using GridSearchCV on Original Training Data) ===\")\n",
    "\n",
    "# --- SVM Tuning ---\n",
    "grid_svm.fit(X_train, y_train)\n",
    "print(\"Best parameters for SVM:\", grid_svm.best_params_)\n",
    "print(\"Best cross-validation accuracy for SVM: {:.4f}\".format(grid_svm.best_score_))\n",
    "\n",
    "# --- Decision Tree Tuning ---\n",
    "grid_dt.fit(X_train, y_train)\n",
    "print(\"Best parameters for Decision Tree:\", grid_dt.best_params_)\n",
    "print(\"Best cross-validation accuracy for Decision Tree: {:.4f}\".format(grid_dt.best_score_))\n",
    "\n",
    "# --- Random Forest Tuning ---\n",
    "grid_rf.fit(X_train, y_train)\n",
    "print(\"Best parameters for Random Forest:\", grid_rf.best_params_)\n",
    "print(\"Best cross-validation accuracy for Random Forest: {:.4f}\".format(grid_rf.best_score_))\n",
    "\n",
    "# Store the best estimators from grid search in a dictionary.\n",
    "tuned_models = {\n",
    "    \"SVM Tuned\": grid_svm.best_estimator_,\n",
    "    \"Decision Tree Tuned\": grid_dt.best_estimator_,\n",
    "    \"Random Forest Tuned\": grid_rf.best_estimator_\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# 8. Evaluate Tuned Models on the Test Set\n",
    "# =============================================================================\n",
    "print(\"\\n=== TEST SET EVALUATION (TUNED MODELS on Original Data) ===\")\n",
    "for name, model in tuned_models.items():\n",
    "    # It is good practice to re-fit the best_estimator_ on the full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    result = evaluate_classifier(model, X_test, y_test, name)\n",
    "    results.append(result)\n",
    "\n",
    "# =============================================================================\n",
    "# 9. Now, Repeat the Experiments on SMOTE-Augmented Training Data\n",
    "# =============================================================================\n",
    "# SMOTE is used to generate synthetic samples of the minority classes so that classes are balanced.\n",
    "smote = SMOTE(random_state=42, k_neighbors=1)  # k_neighbors can be tuned if needed\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "print(\"\\nAfter applying SMOTE, training set class distribution:\")\n",
    "print(pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# ---- 9a. Evaluate Default Models with SMOTE ----\n",
    "print(\"\\n=== CROSS-VALIDATION SCORES (DEFAULT PARAMETERS) on SMOTE Training Data ===\")\n",
    "for name, model in default_models.items():\n",
    "    scores = cross_val_score(model, X_train_smote, y_train_smote, cv=cv, scoring='accuracy')\n",
    "    print(f\"{name} - CV Accuracy (SMOTE): {scores.mean():.4f} (+/- {scores.std():.4f})\")\n",
    "\n",
    "print(\"\\n=== TEST SET EVALUATION (DEFAULT MODELS on SMOTE Data) ===\")\n",
    "for name, model in default_models.items():\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    result = evaluate_classifier(model, X_test, y_test, name + \" (Default + SMOTE)\")\n",
    "    results.append(result)\n",
    "\n",
    "# ---- 9b. Hyperparameter Tuning on SMOTE Data ----\n",
    "print(\"\\n=== HYPERPARAMETER TUNING (Using GridSearchCV on SMOTE Training Data) ===\")\n",
    "\n",
    "# SVM Tuning with SMOTE\n",
    "grid_svm_smote = GridSearchCV(SVC(random_state=42), param_grid_svm, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_svm_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best parameters for SVM with SMOTE:\", grid_svm_smote.best_params_)\n",
    "print(\"Best CV accuracy for SVM with SMOTE: {:.4f}\".format(grid_svm_smote.best_score_))\n",
    "\n",
    "# Decision Tree Tuning with SMOTE\n",
    "grid_dt_smote = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid_dt, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_dt_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best parameters for Decision Tree with SMOTE:\", grid_dt_smote.best_params_)\n",
    "print(\"Best CV accuracy for Decision Tree with SMOTE: {:.4f}\".format(grid_dt_smote.best_score_))\n",
    "\n",
    "# Random Forest Tuning with SMOTE\n",
    "grid_rf_smote = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "grid_rf_smote.fit(X_train_smote, y_train_smote)\n",
    "print(\"Best parameters for Random Forest with SMOTE:\", grid_rf_smote.best_params_)\n",
    "print(\"Best CV accuracy for Random Forest with SMOTE: {:.4f}\".format(grid_rf_smote.best_score_))\n",
    "\n",
    "# Save the best estimators for SMOTE-tuned models\n",
    "tuned_models_smote = {\n",
    "    \"SVM Tuned + SMOTE\": grid_svm_smote.best_estimator_,\n",
    "    \"Decision Tree Tuned + SMOTE\": grid_dt_smote.best_estimator_,\n",
    "    \"Random Forest Tuned + SMOTE\": grid_rf_smote.best_estimator_\n",
    "}\n",
    "\n",
    "print(\"\\n=== TEST SET EVALUATION (TUNED MODELS on SMOTE Data) ===\")\n",
    "for name, model in tuned_models_smote.items():\n",
    "    model.fit(X_train_smote, y_train_smote)\n",
    "    result = evaluate_classifier(model, X_test, y_test, name)\n",
    "    results.append(result)\n",
    "\n",
    "# =============================================================================\n",
    "# 10. Summarize All Results\n",
    "# =============================================================================\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nSummary of classifier performance across all experiments:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assume \"model_rf\" is a trained Random Forest classifier (e.g., one of our tuned models)\n",
    "model_rf = tuned_models_smote[\"Random Forest Tuned + SMOTE\"]\n",
    "\n",
    "# Fit the model (if not already done) on SMOTE data for instance\n",
    "model_rf.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get feature importances\n",
    "importances = model_rf.feature_importances_\n",
    "feature_names = X_train.columns  # assuming X_train is a DataFrame with proper column names\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(importance_df)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Let’s use our tuned Random Forest model as an example:\n",
    "y_pred = model_rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=list(ship_type_mapping.values()),\n",
    "            yticklabels=list(ship_type_mapping.values()))\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix: Random Forest (Tuned + SMOTE)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Check the first few rows and info about the DataFrame:\n",
    "print(\"First 5 rows of the DataFrame:\")\n",
    "display(df.head())\n",
    "print(\"\\nDataFrame info:\")\n",
    "display(df.info())\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Descriptive Statistics\n",
    "# -------------------------------\n",
    "# For numerical features, show summary statistics\n",
    "print(\"\\nSummary statistics for numerical features:\")\n",
    "display(df.describe())\n",
    "\n",
    "# You can also get a summary for categorical columns, if needed:\n",
    "print(\"\\nSummary statistics for categorical features:\")\n",
    "display(df.describe(include=['object']))\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Visualizing the Distribution of Key Features\n",
    "# -------------------------------\n",
    "# Define a list of features you want to analyze.\n",
    "# (Feel free to add or remove features from this list.)\n",
    "features = [\n",
    "    'trip_duration_sec', 'num_positions', 'trajectory_length_km',\n",
    "    'endpoint_distance_km', 'directness_ratio', 'Shape_Complexity',\n",
    "    'Bridge_Position_Ratio', 'total_km2'\n",
    "]\n",
    "\n",
    "# Plot histograms for the selected numerical features\n",
    "df[features].hist(bins=30, figsize=(15, 10))\n",
    "plt.suptitle(\"Histograms of Key Numerical Features\", fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Correlation Analysis\n",
    "# -------------------------------\n",
    "# Compute the correlation matrix for the selected features and plot a heatmap.\n",
    "corr_matrix = df[features].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix of Selected Features\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Pairplot for Exploring Relationships\n",
    "# -------------------------------\n",
    "# Use seaborn's pairplot to see pairwise relationships between the features.\n",
    "sns.pairplot(df[features])\n",
    "plt.suptitle(\"Pairplot of Selected Features\", fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Time Series Analysis: Trips Over Time\n",
    "# -------------------------------\n",
    "# Create a new column for the trip start date (without time) and count trips per day.\n",
    "df['trip_date'] = df['trip_start'].dt.date\n",
    "trips_per_day = df.groupby('trip_date').size()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "trips_per_day.plot(kind='bar')\n",
    "plt.xlabel(\"Trip Start Date\")\n",
    "plt.ylabel(\"Number of Trips\")\n",
    "plt.title(\"Number of Trips Per Day\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Grouped Analysis by Ship Type (or Type of Mobile)\n",
    "# -------------------------------\n",
    "# For example, compare trip duration across different ship types.\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Ship type', y='trip_duration_sec', data=df)\n",
    "plt.title(\"Trip Duration by Ship Type\")\n",
    "plt.xlabel(\"Ship Type\")\n",
    "plt.ylabel(\"Trip Duration (sec)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Similarly, you can analyze other relationships.\n",
    "# For instance, compare trajectory length by mobile type:\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Type of mobile', y='trajectory_length_km', data=df)\n",
    "plt.title(\"Trajectory Length by Type of Mobile\")\n",
    "plt.xlabel(\"Type of Mobile\")\n",
    "plt.ylabel(\"Trajectory Length (km)\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Additional Analysis: Scatter Plots\n",
    "# -------------------------------\n",
    "# Explore the relationship between trip duration and trajectory length.\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='trip_duration_sec', y='trajectory_length_km', data=df, hue='Ship type', palette='viridis')\n",
    "plt.title(\"Trajectory Length vs. Trip Duration\")\n",
    "plt.xlabel(\"Trip Duration (sec)\")\n",
    "plt.ylabel(\"Trajectory Length (km)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Explore the relationship between number of positions and total_km2 covered:\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='num_positions', y='total_km2', data=df, hue='Type of mobile', palette='coolwarm')\n",
    "plt.title(\"Total km2 vs. Number of Positions\")\n",
    "plt.xlabel(\"Number of Positions\")\n",
    "plt.ylabel(\"Total km2\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 9. Conclusion / Next Steps\n",
    "# -------------------------------\n",
    "print(\"Analysis complete. Review the above plots and tables to explore relationships and distributions in your data.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
